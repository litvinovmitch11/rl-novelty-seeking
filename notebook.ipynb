{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Novelty seeking for modeling human behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 1\n",
      "Episode 2: Total Reward = 1\n",
      "Episode 3: Total Reward = 1\n",
      "Episode 4: Total Reward = 1\n",
      "Episode 5: Total Reward = 1\n",
      "Episode 6: Total Reward = 1\n",
      "Episode 7: Total Reward = 1\n",
      "Episode 8: Total Reward = 1\n",
      "Episode 9: Total Reward = 1\n",
      "Episode 10: Total Reward = 1\n",
      "Episode 11: Total Reward = 1\n",
      "Episode 12: Total Reward = 1\n",
      "Episode 13: Total Reward = 1\n",
      "Episode 14: Total Reward = 1\n",
      "Episode 15: Total Reward = 1\n",
      "Episode 16: Total Reward = 1\n",
      "Episode 17: Total Reward = 1\n",
      "Episode 18: Total Reward = 1\n",
      "Episode 19: Total Reward = 1\n",
      "Episode 20: Total Reward = 1\n",
      "Episode 21: Total Reward = 1\n",
      "Episode 22: Total Reward = 1\n",
      "Episode 23: Total Reward = 1\n",
      "Episode 24: Total Reward = 1\n",
      "Episode 25: Total Reward = 1\n",
      "Episode 26: Total Reward = 1\n",
      "Episode 27: Total Reward = 1\n",
      "Episode 28: Total Reward = 1\n",
      "Episode 29: Total Reward = 1\n",
      "Episode 30: Total Reward = 1\n",
      "Episode 31: Total Reward = 1\n",
      "Episode 32: Total Reward = 1\n",
      "Episode 33: Total Reward = 1\n",
      "Episode 34: Total Reward = 1\n",
      "Episode 35: Total Reward = 1\n",
      "Episode 36: Total Reward = 1\n",
      "Episode 37: Total Reward = 1\n",
      "Episode 38: Total Reward = 1\n",
      "Episode 39: Total Reward = 1\n",
      "Episode 40: Total Reward = 1\n",
      "Episode 41: Total Reward = 1\n",
      "Episode 42: Total Reward = 1\n",
      "Episode 43: Total Reward = 1\n",
      "Episode 44: Total Reward = 1\n",
      "Episode 45: Total Reward = 1\n",
      "Episode 46: Total Reward = 1\n",
      "Episode 47: Total Reward = 1\n",
      "Episode 48: Total Reward = 1\n",
      "Episode 49: Total Reward = 1\n",
      "Episode 50: Total Reward = 1\n",
      "\n",
      "=== State Visits (новизна) ===\n",
      "[[6.3527e+04 6.1257e+04 6.0936e+04 5.9293e+04 5.6914e+04]\n",
      " [1.6280e+03 1.6440e+03 1.6220e+03 1.6260e+03 1.5830e+03]\n",
      " [4.4000e+01 5.3000e+01 4.7000e+01 3.7000e+01 9.2000e+01]\n",
      " [0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00 5.3000e+01]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 5.0000e+01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Параметры среды\n",
    "GRID_SIZE = 5  # Размер сетки: GRID_SIZE x GRID_SIZE\n",
    "GOAL = (4, 4)  # Цель, куда должен дойти агент\n",
    "START = (0, 0)  # Стартовое положение агента\n",
    "\n",
    "# Параметры RL\n",
    "ALPHA = 0.1  # Скорость обучения\n",
    "GAMMA = 0.9  # Коэффициент дисконтирования\n",
    "EPSILON = 0.1  # Вероятность выбора случайного действия (Exploration rate)\n",
    "NOVELTY_WEIGHT = 0.1  # Вес влияния компонента новизны\n",
    "\n",
    "# Действия агента: вверх, вниз, влево, вправо\n",
    "ACTIONS = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Функция для выполнения действия в среде\n",
    "def take_action(state, action):\n",
    "    x, y = state\n",
    "    if action == 'up' and x > 0:\n",
    "        return (x-1, y)\n",
    "    if action == 'down' and x < GRID_SIZE-1:\n",
    "        return (x+1, y)\n",
    "    if action == 'left' and y > 0:\n",
    "        return (x, y-1)\n",
    "    if action == 'right' and y < GRID_SIZE-1:\n",
    "        return (x, y+1)\n",
    "    return state  # Если действие невозможно, остаемся на месте\n",
    "\n",
    "# Функция для получения награды\n",
    "def get_reward(state):\n",
    "    return 1 if state == GOAL else 0\n",
    "\n",
    "# Инициализация Q-таблицы и таблицы посещаемости\n",
    "Q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "state_visits = np.zeros((GRID_SIZE, GRID_SIZE))  # Для учета \"новизны\"\n",
    "\n",
    "# Выбор действия с учетом ε-жадной стратегии\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < EPSILON:\n",
    "        return random.choice(ACTIONS)  # Случайное действие\n",
    "    else:\n",
    "        x, y = state\n",
    "        # Учет как стандартной Q-ценности, так и новизны\n",
    "        novelty_bonus = NOVELTY_WEIGHT / (1 + state_visits[x, y])\n",
    "        q_values_with_novelty = Q_table[x, y] + novelty_bonus\n",
    "        return ACTIONS[np.argmax(q_values_with_novelty)]  # Жадный выбор действия\n",
    "\n",
    "# Основной цикл обучения\n",
    "EPISODES = 50\n",
    "for episode in range(EPISODES):\n",
    "    state = START\n",
    "    total_reward = 0\n",
    "    \n",
    "    while state != GOAL:\n",
    "        # Выбор действия\n",
    "        action = choose_action(state)\n",
    "        # Выполнение действия и переход в новое состояние\n",
    "        new_state = take_action(state, action)\n",
    "        # Получение награды\n",
    "        reward = get_reward(new_state)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Обновление таблицы посещений\n",
    "        x, y = new_state\n",
    "        state_visits[x, y] += 1  # Увеличиваем количество посещений состояния\n",
    "        \n",
    "        # Обновление Q-таблицы\n",
    "        old_value = Q_table[state[0], state[1], ACTIONS.index(action)]\n",
    "        next_max = np.max(Q_table[new_state[0], new_state[1]])\n",
    "        Q_table[state[0], state[1], ACTIONS.index(action)] = \\\n",
    "            old_value + ALPHA * (reward + GAMMA * next_max - old_value)\n",
    "        \n",
    "        # Переход в новое состояние\n",
    "        state = new_state\n",
    "\n",
    "    print(f\"Episode {episode+1}: Total Reward = {total_reward}\")\n",
    "\n",
    "# Демонстрация таблиц посетителей (учет новизны)\n",
    "print(\"\\n=== State Visits (новизна) ===\")\n",
    "print(state_visits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m action \u001b[38;5;241m=\u001b[39m choose_action(state)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Выполняем действие и переходим в новое состояние\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Обработка нового формата Gym (возвращается кортеж: (next_state, reward, done, info))\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step_output, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/mitchell-dir/school/rl-novelty-seeking/venv/lib/python3.13/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/mitchell-dir/school/rl-novelty-seeking/venv/lib/python3.13/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mitchell-dir/school/rl-novelty-seeking/venv/lib/python3.13/site-packages/gym/wrappers/env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/mitchell-dir/school/rl-novelty-seeking/venv/lib/python3.13/site-packages/gym/utils/passive_env_checker.py:233\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    230\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool8\u001b[49m)):\n\u001b[1;32m    234\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool8)):\n",
      "File \u001b[0;32m~/mitchell-dir/school/rl-novelty-seeking/venv/lib/python3.13/site-packages/numpy/__init__.py:414\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mchar\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m char\u001b[38;5;241m.\u001b[39mchararray\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Гиперпараметры\n",
    "ALPHA = 0.1  # Скорость обучения\n",
    "GAMMA = 0.99  # Коэффициент дисконтирования\n",
    "EPSILON = 0.1  # Эпсилон для ε-жадной стратегии\n",
    "NOVELTY_WEIGHT = 0.01  # Вес новизны\n",
    "DISCRETE_BINS = (20, 20)  # Количество разбиений (дискретизация) для состояния\n",
    "EPISODES = 5000  # Количество эпизодов для обучения\n",
    "\n",
    "# Создаем среду\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# Функция дискретизации непрерывных состояний\n",
    "def discretize_state(state, env_low, env_high, bins=DISCRETE_BINS):\n",
    "    ratios = (state - env_low) / (env_high - env_low)\n",
    "    discrete_state = (ratios * np.array(bins)).astype(int)\n",
    "    return tuple(np.clip(discrete_state, 0, np.array(bins) - 1))\n",
    "\n",
    "# Инициализация Q-таблицы\n",
    "state_space_bins = DISCRETE_BINS\n",
    "action_space = env.action_space.n\n",
    "Q_table = np.zeros(state_space_bins + (action_space,))\n",
    "\n",
    "# Таблица для учета количества посещений (новизна)\n",
    "state_visits = defaultdict(int)\n",
    "\n",
    "# Функция выбора действия с учетом ε-жадного подхода и компонента новизны\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < EPSILON:\n",
    "        return env.action_space.sample()  # Выбираем случайное действие\n",
    "    else:\n",
    "        novelty_bonus = np.array(\n",
    "            [NOVELTY_WEIGHT / (1 + state_visits[state + (a,)]) for a in range(action_space)]\n",
    "        )\n",
    "        q_values_with_novelty = Q_table[state] + novelty_bonus\n",
    "        return np.argmax(q_values_with_novelty)  # Выбираем действие с наилучшей скорректированной ценностью\n",
    "\n",
    "# Основной цикл обучения\n",
    "for episode in range(EPISODES):\n",
    "    # Сбрасываем среду\n",
    "    reset_output = env.reset()\n",
    "    \n",
    "    # Обработка нового формата Gym (означает, если env.reset() возвращает доп. значения, как словарь)\n",
    "    if isinstance(reset_output, tuple): \n",
    "        state_raw, _ = reset_output\n",
    "    else:  \n",
    "        state_raw = reset_output\n",
    "\n",
    "    state = discretize_state(state_raw, env.observation_space.low, env.observation_space.high)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    while not done:\n",
    "        step += 1\n",
    "        # Выбор действия\n",
    "        action = choose_action(state)\n",
    "        \n",
    "        # Выполняем действие и переходим в новое состояние\n",
    "        step_output = env.step(action)\n",
    "        \n",
    "        # Обработка нового формата Gym (возвращается кортеж: (next_state, reward, done, info))\n",
    "        if isinstance(step_output, tuple):\n",
    "            next_state_raw, reward, done, _ = step_output\n",
    "        else:\n",
    "            next_state_raw = step_output\n",
    "        \n",
    "        next_state = discretize_state(next_state_raw, env.observation_space.low, env.observation_space.high)\n",
    "        \n",
    "        # Награда модифицируется для поощрения поиска новизны\n",
    "        if done and next_state_raw[0] >= env.goal_position:  # Если цель достигнута\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward -= 0.1  # Небольшое наказание за каждый шаг для улучшения обучения\n",
    "\n",
    "        # Обновляем таблицу посещений\n",
    "        state_visits[state + (action,)] += 1\n",
    "\n",
    "        # Обновляем Q-значения\n",
    "        old_value = Q_table[state + (action,)]\n",
    "        next_max = np.max(Q_table[next_state])\n",
    "        Q_table[state + (action,)] = old_value + ALPHA * (reward + GAMMA * next_max - old_value)\n",
    "\n",
    "        # Переходим в новое состояние\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "            \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode: {episode}, Reward: {total_reward}, Steps: {step}\")\n",
    "\n",
    "# Закрытие среды\n",
    "env.close()\n",
    "\n",
    "# Демонстрация количества посещений состояний\n",
    "print(\"\\n=== State-Action Visits ===\")\n",
    "for key, value in list(state_visits.items())[:10]:  # Показываем первые 10 записей\n",
    "    print(f\"State-Action: {key}, Visits: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

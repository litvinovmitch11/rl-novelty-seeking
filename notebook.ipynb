{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZOJHDXrFfZ9"
      },
      "source": [
        "# Reinforcement Learning with Novelty seeking for modeling human behavior"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "Ni_K0duDxZwk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "boBZNqaRFfaA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c538ebb7-ddff-4481-cc59-06d16c28d6c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.26.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "np.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gym.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Htm_TVMazPEx",
        "outputId": "b536c3bb-7584-4217-b832-19d956c31390"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.25.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oCQIaJWJFfZ_",
        "outputId": "7d9f5657-30d5-46a0-c941-b3cdfb746f8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 100, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 200, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 300, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 400, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 500, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 600, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 700, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 800, Reward: -184.89999999999944, Steps: 170\n",
            "Episode: 900, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 1000, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 1100, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 1200, Reward: -179.39999999999947, Steps: 165\n",
            "Episode: 1300, Reward: -206.89999999999932, Steps: 190\n",
            "Episode: 1400, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 1500, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 1600, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 1700, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 1800, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 1900, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 2000, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 2100, Reward: -162.89999999999955, Steps: 150\n",
            "Episode: 2200, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 2300, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 2400, Reward: -206.89999999999932, Steps: 190\n",
            "Episode: 2500, Reward: -169.49999999999952, Steps: 156\n",
            "Episode: 2600, Reward: -168.39999999999952, Steps: 155\n",
            "Episode: 2700, Reward: -214.59999999999928, Steps: 197\n",
            "Episode: 2800, Reward: -203.59999999999934, Steps: 187\n",
            "Episode: 2900, Reward: -181.59999999999945, Steps: 167\n",
            "Episode: 3000, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 3100, Reward: -166.19999999999953, Steps: 153\n",
            "Episode: 3200, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 3300, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 3400, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 3500, Reward: -163.99999999999955, Steps: 151\n",
            "Episode: 3600, Reward: -201.39999999999935, Steps: 185\n",
            "Episode: 3700, Reward: -200.29999999999936, Steps: 184\n",
            "Episode: 3800, Reward: -205.79999999999933, Steps: 189\n",
            "Episode: 3900, Reward: -202.49999999999935, Steps: 186\n",
            "Episode: 4000, Reward: -203.59999999999934, Steps: 187\n",
            "Episode: 4100, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 4200, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 4300, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 4400, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 4500, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 4600, Reward: -207.99999999999932, Steps: 191\n",
            "Episode: 4700, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 4800, Reward: -219.99999999999926, Steps: 200\n",
            "Episode: 4900, Reward: -167.29999999999953, Steps: 154\n",
            "\n",
            "=== State-Action Visits ===\n",
            "State-Action: (7, 0, 0), Visits: 922\n",
            "State-Action: (7, 0, 1), Visits: 770\n",
            "State-Action: (7, 0, 2), Visits: 626\n",
            "State-Action: (7, 9, 0), Visits: 14561\n",
            "State-Action: (7, 9, 1), Visits: 12183\n",
            "State-Action: (7, 9, 2), Visits: 12068\n",
            "State-Action: (7, 10, 0), Visits: 12436\n",
            "State-Action: (7, 10, 1), Visits: 11684\n",
            "State-Action: (7, 10, 2), Visits: 13200\n",
            "State-Action: (8, 10, 0), Visits: 4678\n"
          ]
        }
      ],
      "source": [
        "# Гиперпараметры\n",
        "ALPHA = 0.1  # Скорость обучения\n",
        "GAMMA = 0.9  # Коэффициент дисконтирования\n",
        "EPSILON = 0.1  # Эпсилон для ε-жадной стратегии\n",
        "NOVELTY_WEIGHT = 0.5  # Вес новизны\n",
        "DISCRETE_BINS = (20, 20)  # Количество разбиений (дискретизация) для состояния\n",
        "EPISODES = 5000  # Количество эпизодов для обучения\n",
        "\n",
        "# Создаем среду\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Функция дискретизации непрерывных состояний\n",
        "def discretize_state(state, env_low, env_high, bins=DISCRETE_BINS):\n",
        "    ratios = (state - env_low) / (env_high - env_low)\n",
        "    discrete_state = (ratios * np.array(bins)).astype(int)\n",
        "    return tuple(np.clip(discrete_state, 0, np.array(bins) - 1))\n",
        "\n",
        "# Инициализация Q-таблицы\n",
        "state_space_bins = DISCRETE_BINS\n",
        "action_space = env.action_space.n\n",
        "Q_table = np.zeros(state_space_bins + (action_space,))\n",
        "\n",
        "# Таблица для учета количества посещений (новизна)\n",
        "state_visits = defaultdict(int)\n",
        "\n",
        "# Функция выбора действия с учетом ε-жадного подхода и компонента новизны\n",
        "def choose_action(state):\n",
        "    if random.uniform(0, 1) < EPSILON:\n",
        "        return env.action_space.sample()  # Выбираем случайное действие\n",
        "    else:\n",
        "        novelty_bonus = np.array(\n",
        "            [NOVELTY_WEIGHT / (1 + state_visits[state + (a,)]) for a in range(action_space)]\n",
        "        )\n",
        "        q_values_with_novelty = Q_table[state] + novelty_bonus\n",
        "        return np.argmax(q_values_with_novelty)  # Выбираем действие с наилучшей скорректированной ценностью\n",
        "\n",
        "# Основной цикл обучения\n",
        "for episode in range(EPISODES):\n",
        "    # Сбрасываем среду\n",
        "    state_raw, _ = env.reset()\n",
        "\n",
        "    state = discretize_state(state_raw, env.observation_space.low, env.observation_space.high)\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    step = 0\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "        # Выбор действия\n",
        "        action = choose_action(state)\n",
        "\n",
        "        # Выполняем действие и переходим в новое состояние\n",
        "        next_state_raw, reward, done, _ = env.step(action)\n",
        "\n",
        "        next_state = discretize_state(next_state_raw, env.observation_space.low, env.observation_space.high)\n",
        "\n",
        "        # Награда модифицируется для поощрения поиска новизны\n",
        "        if done and next_state_raw[0] >= env.goal_position:  # Если цель достигнута\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward -= 0.1  # Небольшое наказание за каждый шаг для улучшения обучения\n",
        "\n",
        "        # Обновляем таблицу посещений\n",
        "        state_visits[state + (action,)] += 1\n",
        "\n",
        "        # Обновляем Q-значения\n",
        "        old_value = Q_table[state + (action,)]\n",
        "        next_max = np.max(Q_table[next_state])\n",
        "        Q_table[state + (action,)] = old_value + ALPHA * (reward + GAMMA * next_max - old_value)\n",
        "\n",
        "        # Переходим в новое состояние\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode: {episode}, Reward: {total_reward}, Steps: {step}\")\n",
        "\n",
        "# Закрытие среды\n",
        "env.close()\n",
        "\n",
        "# Демонстрация количества посещений состояний\n",
        "print(\"\\n=== State-Action Visits ===\")\n",
        "for key, value in list(state_visits.items())[:10]:  # Показываем первые 10 записей\n",
        "    print(f\"State-Action: {key}, Visits: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xecEUV_vzYXa"
      },
      "execution_count": 4,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}